{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1: Dataset Class and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PalmsDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir,transform=transform)\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PalmsDataset(\"train\",transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset vs Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image, label in train_set:\n",
    "    break\n",
    "\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(batch_size=32, dataset=train_set, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "Dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2: Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PalmClassifier(nn.Module):\n",
    "    def __init__(self, number_of_classes=9):\n",
    "        super(PalmClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "        # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(256*14*14, 512)\n",
    "        # self.fc2 = nn.Linear(512,number_of_classes)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, number_of_classes)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        # x = F.relu(self.bn4(self.conv4(x)))\n",
    "        # x = self.pool(x)\n",
    "        x = self.gap(x)\n",
    "        \n",
    "        # x = x.view(-1, 256 * 14 * 14)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PalmClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4466e-01, -2.1185e-01,  8.0985e-01, -3.4444e-01,  5.7754e-01,\n",
       "          2.6155e-01,  1.8683e-01, -5.6259e-01,  3.9111e-01],\n",
       "        [-5.0799e-01, -7.5958e-01,  5.2727e-01,  2.0145e-01,  1.0653e+00,\n",
       "         -4.1785e-01,  1.8691e-02, -1.1615e-02,  5.4840e-01],\n",
       "        [-1.3935e-01, -1.7275e+00,  1.9763e+00, -5.3913e-01,  1.6228e+00,\n",
       "          4.9126e-01, -5.7103e-02, -1.2463e+00,  6.9972e-02],\n",
       "        [-2.5647e-02, -1.2291e-01,  9.8184e-01, -8.2864e-01, -1.9743e-02,\n",
       "          7.1930e-01,  2.7903e-01,  8.1783e-02,  1.4960e+00],\n",
       "        [-8.4101e-02, -3.0808e-01,  2.3071e-01, -2.1043e-01,  8.0294e-01,\n",
       "          7.0575e-01,  1.5922e-01, -2.8176e-01,  4.3374e-01],\n",
       "        [-6.2572e-02,  5.5191e-01,  4.3906e-01,  1.2550e-01,  7.0677e-01,\n",
       "          4.7213e-01, -9.4063e-02,  7.7910e-01,  8.3635e-01],\n",
       "        [-9.3516e-02,  1.6707e-01,  1.1808e-01, -6.8341e-01,  1.3686e+00,\n",
       "          1.8557e-01, -1.1320e+00, -9.0969e-01,  6.9037e-01],\n",
       "        [-4.0727e-01, -4.4076e-01,  4.6988e-01, -6.6356e-02, -4.2456e-02,\n",
       "          3.6023e-01,  4.1654e-01, -1.3789e-01,  1.5425e-02],\n",
       "        [-2.6010e-01, -1.6739e-01,  4.3966e-01, -3.0264e-01,  4.6725e-01,\n",
       "          4.3878e-03,  2.2457e-01, -3.2582e-02,  1.0362e+00],\n",
       "        [-4.2988e-01, -3.9224e-01, -1.2267e-01,  2.4824e-01,  9.7891e-01,\n",
       "          1.9232e-01,  5.1192e-02, -2.2985e-01,  1.3136e-01],\n",
       "        [ 2.5194e-01, -5.9306e-01, -8.2447e-02, -9.0289e-01,  2.3322e-01,\n",
       "         -6.1780e-01, -3.7941e-01, -7.1396e-01,  6.0563e-01],\n",
       "        [-1.2277e+00, -2.2107e-01,  9.6965e-01, -1.9386e-02,  9.2428e-01,\n",
       "          1.1430e-01, -1.0261e-01,  1.5025e-01,  1.0574e+00],\n",
       "        [ 1.5753e-01,  2.5353e-01,  1.1915e+00,  1.2080e-01,  1.8419e+00,\n",
       "         -4.5969e-01,  1.1195e-01, -1.2014e+00,  2.1293e+00],\n",
       "        [ 3.1673e-02, -1.8048e-01,  7.2389e-01, -1.8826e-01,  1.1594e+00,\n",
       "          2.4943e-01, -2.2202e-01, -2.5306e-01,  7.5585e-01],\n",
       "        [-3.3635e-01,  1.3224e-01,  6.8985e-01, -3.7797e-01,  7.7545e-01,\n",
       "          1.6149e-01,  3.5686e-01, -1.1969e-01, -4.2967e-01],\n",
       "        [-9.8797e-01, -5.1748e-01,  7.7668e-01, -1.3980e-01,  5.6444e-01,\n",
       "          3.6580e-01,  2.1415e-02, -1.1114e+00, -2.9785e-01],\n",
       "        [-1.2791e-01, -7.0134e-02,  1.2358e-01, -2.3251e-01,  4.3855e-01,\n",
       "          1.2213e+00,  1.0821e+00,  8.7043e-01,  1.5830e-01],\n",
       "        [-2.0710e-01, -8.6431e-01,  5.3009e-01, -2.7664e-01,  9.7820e-01,\n",
       "          7.2564e-02,  4.2417e-01,  1.1356e-01,  3.1103e-01],\n",
       "        [-3.9391e-01, -7.1603e-01,  3.6611e-01,  2.4122e-01,  6.9218e-01,\n",
       "          4.3223e-01, -4.1756e-01, -3.2433e-01, -5.1834e-03],\n",
       "        [-5.7714e-02, -1.8207e-01,  6.4088e-01,  1.5045e-01,  3.7784e-01,\n",
       "          6.3319e-01, -9.0283e-01, -9.9983e-02,  6.3586e-02],\n",
       "        [-4.8284e-01, -9.1126e-02,  7.8426e-02, -4.3305e-01,  3.7740e-01,\n",
       "         -6.2060e-02, -1.4682e-01,  4.3396e-02,  3.6745e-01],\n",
       "        [ 1.2913e-01, -1.4477e-01,  1.4144e+00,  2.4941e-01,  6.2293e-01,\n",
       "         -2.4837e-02, -3.5553e-01, -1.9588e-01,  5.5039e-01],\n",
       "        [-7.0404e-01,  1.4957e-01,  5.0562e-01, -4.6068e-01,  1.1867e+00,\n",
       "          6.3158e-01,  1.1792e-02,  4.1953e-01,  9.5947e-01],\n",
       "        [-4.4050e-01, -4.4536e-01,  1.1359e-01,  1.2654e-01,  5.5644e-01,\n",
       "         -5.0719e-01,  6.3114e-01, -5.0393e-01,  5.6115e-01],\n",
       "        [ 1.0019e-01, -5.9875e-01, -5.9202e-02,  7.0506e-03,  4.1479e-01,\n",
       "          2.0620e-01, -1.8334e-01,  5.4086e-01,  6.6882e-01],\n",
       "        [ 6.4651e-01,  2.6877e-03,  1.0291e-01, -9.4945e-01, -2.1894e-01,\n",
       "          9.4958e-02,  8.2109e-01, -2.3066e-01,  8.2502e-01],\n",
       "        [-6.3578e-01, -3.7300e-01,  6.1013e-01, -5.4554e-01,  6.6481e-01,\n",
       "          5.2076e-01, -4.8086e-01, -5.2121e-01, -1.9681e-01],\n",
       "        [-4.7303e-01, -1.3605e-01,  3.4039e-01, -5.5046e-01,  5.7641e-01,\n",
       "         -1.1926e-01, -2.7727e-02, -9.4476e-01,  1.9434e-01],\n",
       "        [-9.6588e-01, -4.7156e-01,  4.0446e-01, -1.3063e+00,  1.1106e+00,\n",
       "          8.3519e-01, -7.4125e-01, -5.2532e-01,  1.2299e+00],\n",
       "        [-2.9847e-01, -2.8852e-01,  2.6322e-01, -6.6514e-01,  9.6593e-01,\n",
       "          2.0198e-01,  4.3879e-01,  8.5970e-02,  1.6225e-01],\n",
       "        [ 2.7006e-01, -8.3543e-01,  7.7187e-01,  3.1285e-03,  5.8367e-01,\n",
       "          7.4212e-01, -9.8771e-02, -1.3077e-01, -2.8811e-01],\n",
       "        [-3.7596e-01, -4.3614e-01,  1.2868e-01, -1.0879e-01,  1.0865e+00,\n",
       "          1.0539e-01,  4.7221e-02, -5.1459e-01,  2.3435e-04]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_out = model(images)\n",
    "ex_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PalmClassifier(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=9, bias=True)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PalmsDataset(\"valid\", transform=transform)\n",
    "test_dataset = PalmsDataset(\"test\", transform=transform)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Black Scorch': 0,\n",
       " 'Fusarium Wilt': 1,\n",
       " 'Healthy sample': 2,\n",
       " 'Leaf Spots': 3,\n",
       " 'Magnesium Deficiency': 4,\n",
       " 'Manganese Deficiency': 5,\n",
       " 'Parlatoria Blanchardi': 6,\n",
       " 'Potassium Deficiency': 7,\n",
       " 'Rachis Blight': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PalmClassifier(number_of_classes=9).to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode=\"max\",\n",
    "                                                 patience=3,\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=25):\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        \n",
    "        # Update LR scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_palm_cnn.pth')\n",
    "            print(f\"Saved new best model (Acc: {best_acc:.2f}%)\")\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "model.eval()\n",
    "y_test = []\n",
    "predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"testing\"):\n",
    "        y_test.append(labels)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.append(pred)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_state_dict(torch.load())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
